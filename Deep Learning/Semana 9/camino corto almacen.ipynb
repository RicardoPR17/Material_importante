{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "enviroment_rows = 11\n",
    "enviroment_columns = 11\n",
    "\n",
    "q_values = np.zeros((enviroment_rows, enviroment_columns, 4))\n",
    "\n",
    "# definir acciones como posiciones del arreglo\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
      "[-100.   -1. -100. -100. -100. -100. -100.   -1. -100.   -1. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.   -1. -100.]\n",
      "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-100. -100. -100. -100. -100.   -1. -100. -100. -100. -100. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
      "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
     ]
    }
   ],
   "source": [
    "# Recompensas en general, incluyendo estantes para que no tome esa opción\n",
    "rewards = np.full((enviroment_rows, enviroment_columns), -100.)\n",
    "\n",
    "# ubicacions de aisles\n",
    "aisles = {}\n",
    "aisles[1] = [i for i in range(1, 10)]\n",
    "aisles[2] = [1, 7, 9]\n",
    "aisles[3] = [i for i in range(1, 8)]\n",
    "aisles[3].append(9)\n",
    "aisles[4] = [3, 7]\n",
    "aisles[5] = [i for i in range(11)]\n",
    "aisles[6] = [5]\n",
    "aisles[7] = [i for i in range(1, 10)]\n",
    "aisles[8] = [3, 7]\n",
    "aisles[9] = [i for i in range(11)]\n",
    "\n",
    "# Asignar recompensa en los pasillos o donde si puede pasar\n",
    "for row_index in range(1, 10):\n",
    "  for column_index in aisles[row_index]:\n",
    "    rewards[row_index, column_index] = -1.\n",
    "\n",
    "for row in rewards:\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de apoyo\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "  if rewards[current_row_index, current_column_index] == -1.:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "def get_starting_location():\n",
    "  current_row_index = np.random.randint(enviroment_rows)\n",
    "  current_column_index = np.random.randint(enviroment_columns)\n",
    "\n",
    "  while is_terminal_state(current_row_index, current_column_index):\n",
    "    current_row_index = np.random.randint(enviroment_rows)\n",
    "    current_column_index = np.random.randint(enviroment_columns)\n",
    "  \n",
    "  return current_row_index, current_column_index\n",
    "\n",
    "# Epsilon greedy alg\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "  if np.random.random() < epsilon:\n",
    "    return np.argmax(q_values[current_row_index, current_column_index])\n",
    "  else:\n",
    "    return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "  new_row_index = current_row_index\n",
    "  new_column_index = current_column_index\n",
    "\n",
    "  if actions[action_index] == 'up' and current_row_index > 0:\n",
    "    new_row_index -= 1\n",
    "  elif actions[action_index] == 'right' and current_column_index < enviroment_columns - 1:\n",
    "    new_column_index += 1\n",
    "  elif actions[action_index] == 'down' and current_row_index < enviroment_rows - 1:\n",
    "    new_row_index += 1\n",
    "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "    new_column_index -= 1\n",
    "\n",
    "  return new_row_index, new_column_index\n",
    "\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  # Retorno inmediato si inicia en una posición inválida\n",
    "  if is_terminal_state(start_row_index, start_column_index):\n",
    "    return []\n",
    "  else:\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    shortest_path = []\n",
    "    shortest_path.append([start_row_index, start_column_index])\n",
    "\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "      # Obtener el mejor movimiento\n",
    "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "\n",
    "      # moverse a la siguiente posición y añadirla a la lista de movimientos\n",
    "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "    \n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.9 # Porcentaje de tiempo en que debemos tomar la mejor decisión\n",
    "discount_factor = 0.9 # Descuento para futuras recompensas\n",
    "learning_rate = 0.9\n",
    "\n",
    "for episode in range(1000):\n",
    "  row_index, column_index = get_starting_location()\n",
    "\n",
    "  while not is_terminal_state(row_index, column_index):\n",
    "    action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "    old_row_index, old_column_index = row_index, column_index\n",
    "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "    reward = rewards[row_index, column_index]\n",
    "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_shortest_path(3, 9))\n",
    "print(get_shortest_path(5, 0))\n",
    "print(get_shortest_path(9, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
